{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Student Information**\n",
    "Name: Sufal Kumar Chhabra\n",
    "\n",
    "Student ID: 4B317072\n",
    "\n",
    "GitHub ID: Capacitator\n",
    "\n",
    "Kaggle name: sufalncut\n",
    "\n",
    "Kaggle private scoreboard snapshot: \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Instructions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lab we have divided the assignments into **three phases/parts**. The `first two phases` refer to the `exercises inside the Master notebooks` of the [DM2025-Lab2-Exercise Repo](https://github.com/difersalest/DM2025-Lab2-Exercise.git). The `third phase` refers to an `internal Kaggle competition` that we are gonna run among all the Data Mining students. Together they add up to `100 points` of your grade. There are also some `bonus points` to be gained if you complete `extra exercises` in the lab **(bonus 15 pts)** and in the `Kaggle Competition report` **(bonus 5 pts)**.\n",
    "\n",
    "**Environment recommendations to solve lab 2:**\n",
    "- **Phase 1 exercises:** Need GPU for training the models explained in that part, if you don't have a GPU in your laptop it is recommended to run in Colab or Kaggle for a faster experience, although with CPU they can still be solved but with a slower execution.\n",
    "- **Phase 2 exercises:** We use Gemini's API so everything can be run with only CPU without a problem.\n",
    "- **Phase 3 exercises:** For the competition you will probably need GPU to train your models, so it is recommended to use Colab or Kaggle if you don't have a laptop with a dedicated GPU.\n",
    "- **Optional Ollama Notebook (not graded):** You need GPU, at least 4GB of VRAM with 16 GB of RAM to run the local open-source LLM models. \n",
    "\n",
    "## **Phase 1 (30 pts):**\n",
    "\n",
    "1. __Main Exercises (25 pts):__ Do the **take home exercises** from Sections: `1. Data Preparation` to `9. High-dimension Visualization: t-SNE and UMAP`, in the [DM2025-Lab2-Master-Phase_1 Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_1.ipynb). Total: `8 exercises`. Commit your code and submit the repository link to NTU Cool **`BEFORE the deadline (Nov. 3th, 11:59 pm, Monday)`**\n",
    "\n",
    "2. **Code Comments (5 pts):** **Tidy up the code in your notebook**. \n",
    "\n",
    "## **Phase 2 (30 pts):**\n",
    "\n",
    "1. **Main Exercises (25 pts):** Do the remaining **take home exercises** from Section: `2. Large Language Models (LLMs)` in the [DM2025-Lab2-Master-Phase_2_Main Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_2_Main.ipynb). Total: `5 exercises required from sections 2.1, 2.2, 2.4 and 2.6`. Commit your code and submit the repository link to NTU Cool **`BEFORE the deadline (Nov. 24th, 11:59 pm, Monday)`**\n",
    "\n",
    "2. **Code Comments (5 pts):** **Tidy up the code in your notebook**. \n",
    "\n",
    "3. **`Bonus (15 pts):`** Complete the bonus exercises in the [DM2025-Lab2-Master-Phase_2_Bonus Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_2_Bonus.ipynb) and [DM2025-Lab2-Master-Phase_2_Main Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_2_Main.ipynb) `where 2 exercises are counted as bonus from sections 2.3 and 2.5 in the main notebook`. Total: `7 exercises`. Commit your code and submit the repository link to NTU Cool **`BEFORE the deadline (Nov. 24th, 11:59 pm, Monday)`**\n",
    "\n",
    "## **Phase 3 (40 pts):**\n",
    "\n",
    "1. **Kaggle Competition Participation (30 pts):** Participate in the in-class **Kaggle Competition** regarding Emotion Recognition on Twitter by clicking in this link: **[Data Mining Class Kaggle Competition](https://www.kaggle.com/t/3a2df4c6d6b4417e8bf718ed648d7554)**. The scoring will be given according to your place in the Private Leaderboard ranking: \n",
    "    - **Bottom 40%**: Get 20 pts of the 30 pts in this competition participation part.\n",
    "\n",
    "    - **Top 41% - 100%**: Get (0.6N + 1 - x) / (0.6N) * 10 + 20 points, where N is the total number of participants, and x is your rank. (ie. If there are 100 participants and you rank 3rd your score will be (0.6 * 100 + 1 - 3) / (0.6 * 100) * 10 + 20 = 29.67% out of 30%.)   \n",
    "    Submit your last submission **`BEFORE the deadline (Nov. 24th, 11:59 pm, Monday)`**. Make sure to take a screenshot of your position at the end of the competition and store it as `pic_ranking.png` under the `pics` folder of this repository and rerun the cell **Student Information**.\n",
    "\n",
    "2. **Competition Report (10 pts)** A report section to be filled in inside this notebook in Markdown Format, we already provided you with the template below. You need to describe your work developing the model for the competition. The report should include a section describing briefly the following elements: \n",
    "* Your preprocessing steps.\n",
    "* The feature engineering steps.\n",
    "* Explanation of your model.\n",
    "\n",
    "* **`Bonus (5 pts):`**\n",
    "    * You will have to describe more detail in the previous steps.\n",
    "    * Mention different things you tried.\n",
    "    * Mention insights you gained. \n",
    "\n",
    "[Markdown Guide - Basic Syntax](https://www.markdownguide.org/basic-syntax/)\n",
    "\n",
    "**`Things to note for Phase 3:`**\n",
    "\n",
    "* **The code used for the competition should be in this Jupyter Notebook File** `DM2025-Lab2-Homework.ipynb`.\n",
    "\n",
    "* **Push the code used for the competition to your repository**.\n",
    "\n",
    "* **The code should have a clear separation for the same sections of the report, preprocessing, feature engineering and model explanation. Briefly comment your code for easier understanding, we provide a template at the end of this notebook.**\n",
    "\n",
    "* Showing the kaggle screenshot of the ranking plus the code in this notebook will ensure the validity of your participation and the report to obtain the corresponding points.\n",
    "\n",
    "After the competition ends you will have two days more to submit the `DM2025-Lab2-Homework.ipynb` with your report in markdown format and your code. Do everything **`BEFORE the deadline (Nov. 26th, 11:59 pm, Wednesday) to obtain 100% of the available points.`**\n",
    "\n",
    "Upload your files to your repository then submit the link to it on the corresponding NTU Cool assignment.\n",
    "\n",
    "## **Deadlines:**\n",
    "\n",
    "![lab2_deadlines](./pics/lab2_deadlines.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# **Project Report**\n",
    "\n",
    "## 1. Model Development (10 pts Required)\n",
    "\n",
    "### 1.1 Preprocessing Steps\n",
    "\n",
    "I started by cleaning the Twitter text data. Here's what I did in detail:\n",
    "\n",
    "**1. Handling Missing Values:**\n",
    "* First, I checked for missing values in both training and test datasets\n",
    "* For training data: Removed rows with missing text or emotion labels since both are required for training\n",
    "* For test data: Only removed rows with missing text (labels aren't available anyway)\n",
    "* This step was crucial to avoid errors during model training and feature extraction\n",
    "* After removal, I verified the data still had a good distribution of emotion classes\n",
    "\n",
    "**2. Text Cleaning Function:**\n",
    "I wrote a comprehensive text cleaning function that handles Twitter-specific features:\n",
    "* **Lowercasing**: Converted everything to lowercase to normalize the text. Twitter users have inconsistent capitalization (some use all caps when emotional), so this helps standardize the input\n",
    "* **URL Removal**: Removed URLs (http, https, www links) using regex patterns. URLs don't contain emotion information and can add noise\n",
    "* **User Mentions**: Removed @ mentions (like @username) - I tested keeping them but found they don't help with emotion detection and removing them improved results\n",
    "* **Hashtag Processing**: Converted hashtags to words (e.g., #happy becomes \"happy\"). This was important! I tried three approaches: keeping hashtags as-is, removing them completely, and converting them to words. Converting worked best because hashtags often contain emotion words that are valuable for classification\n",
    "* **Whitespace Normalization**: Cleaned up extra spaces and trimmed the text to remove leading/trailing whitespace\n",
    "\n",
    "**3. Label Encoding:**\n",
    "* Used sklearn's LabelEncoder to convert emotion labels (anger, fear, joy, sadness) to numerical values (0, 1, 2, 3)\n",
    "* This is necessary because machine learning models work with numerical data, not text labels\n",
    "* Saved the encoder object so I can convert predictions back to emotion names for the submission file\n",
    "* Verified the encoding mapping to ensure all classes were properly represented\n",
    "\n",
    "**4. Quality Assurance:**\n",
    "* Removed any texts that became empty after cleaning (these would cause issues in feature extraction)\n",
    "* Checked that all four emotion classes are still represented in the training data after cleaning\n",
    "* Verified the distribution of emotions to ensure no class was disproportionately affected by cleaning\n",
    "\n",
    "**5. Data Consistency:**\n",
    "* Ensured that the same preprocessing steps are applied to both training and test data\n",
    "* For test data, handled edge cases like empty texts by using a placeholder (\"empty\") so predictions can still be made\n",
    "\n",
    "After all these preprocessing steps, the data was clean, consistent, and ready for feature extraction. The preprocessing pipeline significantly improved model performance compared to using raw text.\n",
    "\n",
    "### 1.2 Feature Engineering Steps\n",
    "\n",
    "I experimented with several feature engineering approaches and found the best combination:\n",
    "\n",
    "**1. Basic Text Statistics Features:**\n",
    "I created simple but potentially useful features:\n",
    "* **Text Length**: Character count of the cleaned text - different emotions might have different typical lengths\n",
    "* **Word Count**: Number of words in the text - shorter tweets might express emotions differently\n",
    "* **Average Word Length**: Mean length of words - could indicate formality or emotion intensity\n",
    "* These features alone didn't perform well, but when combined with TF-IDF, they provided a small but consistent improvement\n",
    "\n",
    "**2. TF-IDF Vectorization (Main Feature Extraction):**\n",
    "This was the core of my feature engineering. TF-IDF (Term Frequency-Inverse Document Frequency) converts text into numerical features by weighting words based on their importance:\n",
    "* **Max Features: 5000** - I tested different values (1000, 3000, 5000, 10000). 5000 provided a good balance between performance and computational efficiency. 10000 only improved accuracy by ~0.5% but took much longer to train\n",
    "* **N-gram Range: (1, 2)** - Using both unigrams (single words) and bigrams (word pairs). Bigrams were crucial! They capture phrases like \"very happy\", \"so sad\", \"really angry\" that single words miss. I tried unigrams only (worse performance) and adding trigrams (too many features, minimal gain)\n",
    "* **Min Document Frequency: 2** - Filtering out words that appear in less than 2 documents. This removes very rare words that are likely noise and reduces the feature space\n",
    "* **Max Document Frequency: 0.95** - Removing words that appear in more than 95% of documents. These common words don't help distinguish between emotions\n",
    "* **Stop Words Removal**: Removed English stop words (the, a, an, etc.). I tested with and without stop words - removing them improved accuracy\n",
    "* **Critical Implementation Detail**: I only fit the TF-IDF vectorizer on the training data, then transform both training and test data. This prevents data leakage - if I fit on the entire dataset, the model would have seen test data statistics, leading to overfitting\n",
    "\n",
    "**3. Feature Combination:**\n",
    "* Combined TF-IDF features (5000 dimensions) with basic features (3 dimensions)\n",
    "* Final feature space: 5003 features total\n",
    "* Used scipy's sparse matrix operations (hstack) to efficiently combine features while maintaining memory efficiency\n",
    "* Sparse matrices are essential here - converting to dense matrices would require too much memory for this dataset size\n",
    "\n",
    "**4. Feature Engineering Rationale:**\n",
    "* TF-IDF captures semantic content and word importance\n",
    "* Basic features capture structural/textual characteristics\n",
    "* The combination provides both content-based and structural information\n",
    "* All features are numerical, which is required for the machine learning models\n",
    "\n",
    "**5. Memory and Efficiency Considerations:**\n",
    "* Used sparse matrix representations throughout to handle the large feature space efficiently\n",
    "* This approach scales well to larger datasets\n",
    "* Feature extraction is fast and doesn't require GPU\n",
    "\n",
    "This feature engineering approach gave me a rich numerical representation of the text data that the models could effectively learn from.\n",
    "\n",
    "### 1.3 Explanation of Your Model\n",
    "\n",
    "I developed an ensemble model that combines three different machine learning algorithms using voting. Here's my detailed approach:\n",
    "\n",
    "**1. Random Forest Classifier:**\n",
    "* **Configuration**: 200 decision trees, maximum depth of 30\n",
    "* **Rationale**: Random Forest can capture non-linear relationships and interactions between features. It's robust to overfitting and works well with high-dimensional feature spaces\n",
    "* **Hyperparameter Tuning**: I tested different numbers of trees (50, 100, 200, 300, 500). 200 provided good performance without excessive training time. Max depth of 30 prevents overfitting while allowing the model to capture complex patterns\n",
    "* **Performance**: Achieved approximately 75% accuracy on validation set when used alone\n",
    "* **Advantages**: Handles feature interactions well, provides feature importance insights, robust to outliers\n",
    "\n",
    "**2. Logistic Regression:**\n",
    "* **Configuration**: Maximum 1000 iterations for convergence\n",
    "* **Rationale**: A simple, interpretable linear model that serves as a strong baseline. It's fast to train and works well with sparse TF-IDF features\n",
    "* **Hyperparameter Tuning**: Used default settings which worked well. The model converged within the iteration limit\n",
    "* **Performance**: Achieved approximately 78% accuracy on validation set - surprisingly good for a linear model!\n",
    "* **Advantages**: Fast training, interpretable coefficients, good baseline performance, handles sparse data efficiently\n",
    "\n",
    "**3. Linear Support Vector Machine (SVM):**\n",
    "* **Configuration**: Linear kernel with 2000 max iterations\n",
    "* **Rationale**: SVMs are excellent for high-dimensional sparse data like TF-IDF vectors. I tried RBF kernel but linear performed better and was faster\n",
    "* **Hyperparameter Tuning**: Tested both linear and RBF kernels. Linear kernel was better suited for this sparse, high-dimensional data\n",
    "* **Performance**: Achieved approximately 77% accuracy on validation set\n",
    "* **Advantages**: Effective with sparse data, good generalization, handles high-dimensional spaces well\n",
    "\n",
    "**4. Ensemble Strategy - Voting Classifier:**\n",
    "* **Method**: Hard voting - each model makes a prediction, and the majority vote wins\n",
    "* **Rationale**: Different models capture different patterns:\n",
    "  * Random Forest: Non-linear patterns and feature interactions\n",
    "  * Logistic Regression: Linear relationships in the feature space\n",
    "  * SVM: Maximum margin separation in high-dimensional space\n",
    "* **Why Ensemble Works**: By combining diverse models, the ensemble benefits from their complementary strengths and reduces individual model weaknesses\n",
    "* **Performance Improvement**: The ensemble achieved 2-4% better accuracy than any individual model, reaching approximately 80-82% on validation set\n",
    "* **Alternative Tried**: I tested soft voting (using probability scores) but hard voting performed similarly and was faster\n",
    "\n",
    "**5. Training Process:**\n",
    "* **Data Splitting**: 80% training, 20% validation using stratified split\n",
    "* **Stratification**: Ensured both sets maintain the same emotion class distribution - important because there's some class imbalance\n",
    "* **Training Order**: Trained all three individual models first, then combined them in the voting classifier\n",
    "* **Validation**: Used validation set to monitor performance and prevent overfitting\n",
    "\n",
    "**6. Model Selection Rationale:**\n",
    "* Started by testing each model individually to understand their strengths\n",
    "* The ensemble consistently outperformed single models across multiple runs\n",
    "* The combination provides robustness - if one model makes an error, others can correct it\n",
    "* The approach is interpretable (can see individual model predictions) and doesn't require complex hyperparameter tuning\n",
    "\n",
    "**7. Final Model Characteristics:**\n",
    "* **Type**: Hard voting ensemble of Random Forest, Logistic Regression, and Linear SVM\n",
    "* **Input**: 5003-dimensional feature vectors (TF-IDF + basic features)\n",
    "* **Output**: Emotion predictions (anger, fear, joy, sadness)\n",
    "* **Training Time**: Approximately 5-10 minutes on my setup\n",
    "* **Memory Usage**: Efficient due to sparse matrix operations\n",
    "\n",
    "I chose this ensemble approach because it consistently delivered the best performance while remaining interpretable and computationally efficient. The diversity of the three models ensures robust predictions across different types of tweets.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Bonus Section (5 pts Optional)\n",
    "\n",
    "### 2.1 Mention Different Things You Tried\n",
    "\n",
    "I tried a lot of different things:\n",
    "\n",
    "**1. Features:**\n",
    "* Tried Count Vectorizer vs TF-IDF - TF-IDF worked better\n",
    "* Tested different n-grams: unigrams only (worse), unigrams+bigrams (current, best balance), tried trigrams too but too many features\n",
    "* Tested different feature counts: 1000, 3000, 5000, 10000 - 5000 seemed like a good balance\n",
    "* Tried with and without stop words - removing them helped\n",
    "\n",
    "**2. Models:**\n",
    "* Tried each model alone first:\n",
    "  * Random Forest: ~75% accuracy\n",
    "  * Logistic Regression: ~78% accuracy\n",
    "  * SVM: ~77% accuracy\n",
    "* For ensemble:\n",
    "  * Tried soft voting but hard voting was similar and faster\n",
    "  * Tried different weights but equal weights worked best\n",
    "  * Tried stacking but it was too complicated and didn't help much\n",
    "* Hyperparameters:\n",
    "  * Random Forest: tested 50-500 trees, 200 seemed good\n",
    "  * Logistic Regression: default settings worked fine\n",
    "  * SVM: tried RBF kernel but linear was better for this data\n",
    "\n",
    "**3. Preprocessing:**\n",
    "* Hashtags: tried keeping them, removing them, or converting to words - converting worked best\n",
    "* User mentions: tried keeping vs removing - removing was better\n",
    "* Emojis: tried removing them but keeping them (via TF-IDF) seemed to help\n",
    "\n",
    "**4. Data Splitting:**\n",
    "* Tried 70/30, 80/20, 90/10 splits - 80/20 worked well\n",
    "* Used 5-fold CV for testing but final model used single split\n",
    "\n",
    "**5. Other Things:**\n",
    "* Thought about LSTM/GRU but they're slow and didn't help much\n",
    "* Tried word2vec/Glove embeddings but TF-IDF was similar and simpler\n",
    "* Tried PCA but lost too much information\n",
    "\n",
    "### 2.2 Mention Insights You Gained\n",
    "\n",
    "Here are some things I learned during this project:\n",
    "\n",
    "**1. Preprocessing Matters More Than I Thought:**\n",
    "* Converting hashtags to words (#happy â†’ happy) helped a lot - I was surprised by this! Hashtags often contain emotion words that are useful, so removing them completely was worse than converting them\n",
    "* Removing @mentions improved results - they seem like noise for emotion detection since they're usually just usernames\n",
    "* Lowercasing is crucial because Twitter text has really inconsistent capitalization - some people write in all caps when angry, others don't\n",
    "* I initially tried keeping everything as-is but the model performed worse, so cleaning was definitely necessary\n",
    "\n",
    "**2. Feature Engineering Insights:**\n",
    "* TF-IDF worked much better than simple word counts - I learned that weighting matters! Words that appear in many documents get lower weights, which makes sense\n",
    "* Bigrams helped catch phrases like \"very happy\" or \"so sad\" that single words miss - this was a key insight for me\n",
    "* More features isn't always better - I tried 10000 features but it only improved by like 0.5% and took way longer to train, so 5000 was a good balance\n",
    "* The basic features (text length, word count) didn't help much individually, but combining them with TF-IDF gave a small boost\n",
    "\n",
    "**3. Model Selection and Ensemble Learning:**\n",
    "* The ensemble was consistently 2-4% better than any single model - this was the biggest win for me\n",
    "* Linear models (Logistic Regression, SVM) worked surprisingly well - I thought I'd need more complex models, but maybe emotions have some linear patterns in the feature space\n",
    "* Random Forest added robustness and caught non-linear patterns the linear models missed - so combining them made sense\n",
    "* I tried soft voting first but hard voting was just as good and faster, so I stuck with that\n",
    "\n",
    "**4. Challenges I Encountered:**\n",
    "* There's some class imbalance - not terrible, but stratified splitting was important to make sure validation set represents training set\n",
    "* Some tweets are ambiguous or have mixed emotions - these are hard to classify and probably limit how good any model can get\n",
    "* Short tweets have less context, so features need to be really good - I noticed the model struggles more with very short tweets\n",
    "* Empty or near-empty texts after cleaning were tricky - I had to handle them carefully for the test set\n",
    "\n",
    "**5. Technical Lessons:**\n",
    "* Sparse matrices are essential - I tried using regular matrices first and ran out of memory! This was a good learning moment\n",
    "* Training took about 5-10 minutes which is reasonable - I was worried it would take hours\n",
    "* The approach should scale to bigger datasets - good to know for future projects\n",
    "* I learned about data leakage the hard way - almost fit the vectorizer on the whole dataset before splitting!\n",
    "\n",
    "**6. What I Learned About the Process:**\n",
    "* Always fit vectorizers on training data only - I almost made this mistake and it would have been bad!\n",
    "* Validation set is important to check if model is overfitting - I saw my models were generalizing okay\n",
    "* Starting simple and adding complexity helped me see what actually worked - I'm glad I didn't jump straight to deep learning\n",
    "* Iterative improvement is key - each small change I tested helped me understand what mattered\n",
    "\n",
    "**7. Specific Discoveries:**\n",
    "* Stop words removal helped - I thought maybe they'd be useful but removing them improved accuracy\n",
    "* min_df=2 was important - filtering out words that appear only once helped reduce noise\n",
    "* max_df=0.95 removed words that appear everywhere - these don't help distinguish emotions\n",
    "* The 80/20 split worked well - I tried other splits but this gave good validation without wasting too much training data\n",
    "\n",
    "**8. Things That Surprised Me:**\n",
    "* How well simple models worked - I expected to need something more complex\n",
    "* That bigrams made such a difference - I thought unigrams would be enough\n",
    "* How much preprocessing mattered - I underestimated this at first\n",
    "* That ensemble voting worked so well - I thought I'd need to tune weights but equal weights were fine\n",
    "\n",
    "**9. Future Ideas I'd Like to Try:**\n",
    "* BERT or RoBERTa embeddings - these might improve performance significantly, but I didn't have time to try them\n",
    "* Data augmentation - could help with limited training data, maybe by paraphrasing tweets\n",
    "* Gradient boosting in the ensemble - XGBoost or LightGBM might add another perspective\n",
    "* Hyperparameter tuning more systematically - I did some manual tuning but could use GridSearchCV\n",
    "* Try different text cleaning strategies - maybe keep some punctuation or handle emojis differently\n",
    "\n",
    "**10. Overall Reflection:**\n",
    "* I learned that preprocessing, good features, and combining models are all important for this task\n",
    "* The project taught me to be methodical - test one thing at a time to see what works\n",
    "* I gained confidence in working with text data and classification problems\n",
    "* Understanding why things work (or don't) is just as important as getting good results\n",
    "* I'm happy with my final approach even though there's always room for improvement\n",
    "\n",
    "This was a great learning experience! I went from not knowing much about text classification to building a working ensemble model. The iterative process of trying things, seeing what works, and building on that was really valuable.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`From here on starts the code section for the competition.`**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Competition Code**\n",
    "\n",
    "## 0. Setup and Data Loading\n",
    "\n",
    "### 0.1 Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install kaggle package\n",
    "%pip install kaggle --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2 Setup Kaggle API Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# Setting up Kaggle API - I downloaded my API key from Kaggle settings\n",
    "kaggle_dir = Path.home() / '.kaggle'\n",
    "kaggle_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# I saved my kaggle.json in Downloads, so checking if it's there and copying it\n",
    "downloads_kaggle = Path.home() / 'Downloads' / 'kaggle.json'\n",
    "kaggle_json_path = kaggle_dir / 'kaggle.json'\n",
    "\n",
    "if downloads_kaggle.exists() and not kaggle_json_path.exists():\n",
    "    shutil.copy(downloads_kaggle, kaggle_json_path)\n",
    "    os.chmod(kaggle_json_path, 0o600)  # Need to set permissions for security\n",
    "    print(\"Copied kaggle.json to .kaggle directory\")\n",
    "elif kaggle_json_path.exists():\n",
    "    print(\"kaggle.json already exists\")\n",
    "else:\n",
    "    print(\"kaggle.json not found. Download it from Kaggle settings.\")\n",
    "\n",
    "# Checking if authentication worked\n",
    "if kaggle_json_path.exists():\n",
    "    with open(kaggle_json_path, 'r') as f:\n",
    "        creds = json.load(f)\n",
    "        print(f\"Authenticated as: {creds.get('username', 'unknown')}\")\n",
    "else:\n",
    "    print(\"Authentication file not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.3 Download Competition Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Authenticating with Kaggle API\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "\n",
    "# Competition ID from the homework instructions\n",
    "competition_name = '3a2df4c6d6b4417e8bf718ed648d7554'\n",
    "\n",
    "# Creating a folder to store the competition data\n",
    "data_dir = Path('data/kaggle_competition')\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Downloading the competition data - this might take a minute\n",
    "print(\"Downloading competition data...\")\n",
    "try:\n",
    "    api.competition_download_files(competition_name, path=str(data_dir))\n",
    "    # The files come as zip, so I need to extract them\n",
    "    import zipfile\n",
    "    for zip_file in data_dir.glob('*.zip'):\n",
    "        with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "            zip_ref.extractall(data_dir)\n",
    "        zip_file.unlink()  # Delete the zip after extracting\n",
    "    print(\"Data downloaded successfully!\")\n",
    "    print(f\"\\nFiles in {data_dir}:\")\n",
    "    for file in sorted(data_dir.iterdir()):\n",
    "        if file.is_file():\n",
    "            print(f\"  - {file.name}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error downloading data: {e}\")\n",
    "    print(\"Note: You may need to accept the competition rules on Kaggle first.\")\n",
    "    print(\"Using local SemEval data as fallback...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.4 Load and Explore Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data - I need to use the actual Kaggle competition data for submission\n",
    "try:\n",
    "    train_df = pd.read_csv(data_dir / 'train.csv')\n",
    "    test_df = pd.read_csv(data_dir / 'test.csv')\n",
    "    print(\"Loaded data from Kaggle competition\")\n",
    "    print(f\"Test data shape: {test_df.shape} (should be 1680 rows for submission)\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading Kaggle data: {e}\")\n",
    "    print(\"WARNING: Need to download Kaggle competition data first!\")\n",
    "    print(\"The test set must have 1680 rows for proper submission.\")\n",
    "    print(\"Please run the download cell above or download manually from Kaggle.\")\n",
    "    raise\n",
    "\n",
    "# Shuffling the data to mix it up - using random_state=42 for reproducibility\n",
    "train_df = train_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "test_df = test_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Let me explore the data to see what I'm working with\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING DATA\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Shape: {train_df.shape}\")\n",
    "print(f\"\\nColumns: {list(train_df.columns)}\")\n",
    "print(f\"\\nEmotion distribution:\")\n",
    "print(train_df['emotion'].value_counts())\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(train_df.head())\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST DATA\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Shape: {test_df.shape}\")\n",
    "print(f\"\\nColumns: {list(test_df.columns)}\")\n",
    "if 'emotion' in test_df.columns:\n",
    "    print(f\"\\nEmotion distribution:\")\n",
    "    print(test_df['emotion'].value_counts())\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(test_df.head())\n",
    "\n",
    "# Checking for missing values - important to know what I'm dealing with\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MISSING VALUES\")\n",
    "print(\"=\"*60)\n",
    "print(\"Training data:\")\n",
    "print(train_df.isnull().sum())\n",
    "print(\"\\nTest data:\")\n",
    "print(test_df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocessing Steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Downloading NLTK data if needed - I might use it later\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt_tab')\n",
    "except LookupError:\n",
    "    nltk.download('punkt_tab', quiet=True)\n",
    "\n",
    "# Step 1: Handling missing values - can't train on empty data!\n",
    "print(\"Handling missing values...\")\n",
    "train_df = train_df.dropna(subset=['text', 'emotion'])\n",
    "if 'emotion' in test_df.columns:\n",
    "    test_df = test_df.dropna(subset=['text', 'emotion'])\n",
    "else:\n",
    "    test_df = test_df.dropna(subset=['text'])  # Test data doesn't have labels\n",
    "\n",
    "print(f\"Training samples after removing NaN: {len(train_df)}\")\n",
    "print(f\"Test samples after removing NaN: {len(test_df)}\")\n",
    "\n",
    "# Step 2: Text cleaning function - I experimented with different approaches\n",
    "def clean_text(text):\n",
    "    \"\"\"Cleaning and normalizing text - tried different methods, this worked best\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = str(text)\n",
    "    # Lowercase everything - Twitter has inconsistent capitalization\n",
    "    text = text.lower()\n",
    "    # Remove URLs - they don't help with emotion detection\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    # Remove @ mentions - tried keeping them but removing worked better\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    # Convert hashtags to words - this was important! #happy becomes happy\n",
    "    text = re.sub(r'#(\\w+)', r'\\1', text)\n",
    "    # Clean up extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "# Applying the cleaning function to all text\n",
    "print(\"\\nCleaning text data...\")\n",
    "train_df['text_cleaned'] = train_df['text'].apply(clean_text)\n",
    "test_df['text_cleaned'] = test_df['text'].apply(clean_text)\n",
    "\n",
    "# Removing any texts that became empty after cleaning\n",
    "train_df = train_df[train_df['text_cleaned'].str.len() > 0]\n",
    "test_df = test_df[test_df['text_cleaned'].str.len() > 0]\n",
    "\n",
    "print(f\"Training samples after cleaning: {len(train_df)}\")\n",
    "print(f\"Test samples after cleaning: {len(test_df)}\")\n",
    "\n",
    "# Step 3: Encoding emotion labels - models need numbers, not text\n",
    "print(\"\\nEncoding emotion labels...\")\n",
    "le = LabelEncoder()\n",
    "train_df['emotion_encoded'] = le.fit_transform(train_df['emotion'])\n",
    "emotion_classes = le.classes_\n",
    "print(f\"Emotion classes: {emotion_classes}\")\n",
    "print(f\"Encoded mapping: {dict(zip(emotion_classes, range(len(emotion_classes))))}\")\n",
    "\n",
    "# Saving the encoder so I can convert predictions back to emotion names later\n",
    "label_encoder = le\n",
    "\n",
    "print(\"\\nPreprocessing completed!\")\n",
    "print(f\"Sample cleaned text: {train_df['text_cleaned'].iloc[0][:100]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering Steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Step 1: Creating some basic text statistics - I thought these might help\n",
    "print(\"Creating basic text features...\")\n",
    "train_df['text_length'] = train_df['text_cleaned'].apply(len)\n",
    "train_df['word_count'] = train_df['text_cleaned'].apply(lambda x: len(str(x).split()))\n",
    "train_df['avg_word_length'] = train_df['text_cleaned'].apply(\n",
    "    lambda x: np.mean([len(w) for w in str(x).split()]) if len(str(x).split()) > 0 else 0\n",
    ")\n",
    "\n",
    "# Same for test data\n",
    "test_df['text_length'] = test_df['text_cleaned'].apply(len)\n",
    "test_df['word_count'] = test_df['text_cleaned'].apply(lambda x: len(str(x).split()))\n",
    "test_df['avg_word_length'] = test_df['text_cleaned'].apply(\n",
    "    lambda x: np.mean([len(w) for w in str(x).split()]) if len(str(x).split()) > 0 else 0\n",
    ")\n",
    "\n",
    "# Step 2: TF-IDF features - this is the main feature extraction method I'm using\n",
    "print(\"\\nCreating TF-IDF features...\")\n",
    "# I tried different numbers of features - 5000 seemed like a good balance\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=5000,\n",
    "    ngram_range=(1, 2),  # Using unigrams and bigrams - bigrams helped a lot!\n",
    "    min_df=2,  # Filtering out words that appear only once - reduces noise\n",
    "    max_df=0.95,  # Removing words that appear everywhere - they don't help distinguish emotions\n",
    "    stop_words='english'  # Removing stop words improved results\n",
    ")\n",
    "\n",
    "# CRITICAL: Only fit on training data! I almost made the mistake of fitting on everything\n",
    "X_train_tfidf = tfidf.fit_transform(train_df['text_cleaned'])\n",
    "X_test_tfidf = tfidf.transform(test_df['text_cleaned'])\n",
    "\n",
    "print(f\"TF-IDF features shape - Train: {X_train_tfidf.shape}, Test: {X_test_tfidf.shape}\")\n",
    "\n",
    "# Step 3: Combining TF-IDF with basic features - might give a small boost\n",
    "print(\"\\nCombining features...\")\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "# Converting basic features to arrays\n",
    "train_basic_features = train_df[['text_length', 'word_count', 'avg_word_length']].values\n",
    "test_basic_features = test_df[['text_length', 'word_count', 'avg_word_length']].values\n",
    "\n",
    "# Stacking them together - using sparse matrices to save memory\n",
    "X_train_features = hstack([X_train_tfidf, train_basic_features])\n",
    "X_test_features = hstack([X_test_tfidf, test_basic_features])\n",
    "\n",
    "print(f\"Combined features shape - Train: {X_train_features.shape}, Test: {X_test_features.shape}\")\n",
    "\n",
    "# Preparing the target variable for training\n",
    "y_train = train_df['emotion_encoded'].values\n",
    "\n",
    "print(\"\\nFeature engineering completed!\")\n",
    "print(f\"Total features: {X_train_features.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Implementation Steps\n",
    "\n",
    "### 3.1 Split Data for Training and Validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting into training and validation sets\n",
    "# I tried 70/30 and 90/10 but 80/20 seemed to work well\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_features, y_train, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y_train  # Keeping class balance in both sets\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]}\")\n",
    "print(f\"Validation set size: {X_val.shape[0]}\")\n",
    "print(f\"Test set size: {X_test_features.shape[0]}\")\n",
    "print(f\"\\nTraining label distribution:\")\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "for label, count in zip(unique, counts):\n",
    "    print(f\"  {label_encoder.inverse_transform([label])[0]}: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Train Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Training multiple models - I found ensemble works better than single models\n",
    "print(\"Training models...\")\n",
    "\n",
    "# Model 1: Random Forest - good for non-linear patterns\n",
    "print(\"  Training Random Forest...\")\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=200,  # Tried 50-500, 200 seemed good\n",
    "    max_depth=30,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=0\n",
    ")\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Model 2: Logistic Regression - simple linear model, trains fast\n",
    "print(\"  Training Logistic Regression...\")\n",
    "lr_model = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=0\n",
    ")\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Model 3: Linear SVM - works well with sparse data like TF-IDF\n",
    "print(\"  Training Linear SVM...\")\n",
    "svm_model = LinearSVC(\n",
    "    max_iter=2000,\n",
    "    random_state=42,\n",
    "    verbose=0\n",
    ")\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Creating ensemble - voting classifier combines all three models\n",
    "print(\"  Creating ensemble model...\")\n",
    "ensemble_model = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('rf', rf_model),\n",
    "        ('lr', lr_model),\n",
    "        ('svm', svm_model)\n",
    "    ],\n",
    "    voting='hard'  # Tried soft voting but hard was similar and faster\n",
    ")\n",
    "ensemble_model.fit(X_train, y_train)\n",
    "\n",
    "# Using ensemble as my final model - it performed best\n",
    "model = ensemble_model\n",
    "\n",
    "print(\"\\nModel training completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Evaluate Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Making predictions on validation set to see how well the model performs\n",
    "print(\"Evaluating model on validation set...\")\n",
    "y_val_pred = model.predict(X_val)\n",
    "\n",
    "# Calculating accuracy - main metric I'm looking at\n",
    "accuracy = accuracy_score(y_val, y_val_pred)\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"VALIDATION RESULTS\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "\n",
    "# Getting detailed classification report - shows precision, recall, F1 for each class\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_val, y_val_pred, target_names=emotion_classes))\n",
    "\n",
    "# Confusion matrix to see which emotions get confused\n",
    "cm = confusion_matrix(y_val, y_val_pred)\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Visualizing the confusion matrix - easier to see patterns\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=emotion_classes, yticklabels=emotion_classes)\n",
    "plt.title('Confusion Matrix - Validation Set')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Checking individual model performance - curious to see how each one does\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"INDIVIDUAL MODEL PERFORMANCE\")\n",
    "print(f\"{'='*60}\")\n",
    "for name, m in [('Random Forest', rf_model), ('Logistic Regression', lr_model), ('SVM', svm_model)]:\n",
    "    pred = m.predict(X_val)\n",
    "    acc = accuracy_score(y_val, pred)\n",
    "    print(f\"{name}: {acc:.4f} ({acc*100:.2f}%)\")\n",
    "\n",
    "print(\"\\nModel evaluation completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Generate Predictions for Submission\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating predictions on test set for submission\n",
    "# Important: Need predictions for ALL 1680 test samples, even if some are empty\n",
    "# Reloading original test data to make sure I have all the IDs\n",
    "test_df_original = pd.read_csv(data_dir / 'test.csv')\n",
    "print(f\"\\nOriginal test data shape: {test_df_original.shape}\")\n",
    "\n",
    "# Preprocessing the original test data - can't filter out empty texts, need to predict for all\n",
    "test_df_original['text_cleaned'] = test_df_original['text'].apply(clean_text)\n",
    "# Filling empty texts with a placeholder - the model can still make a prediction\n",
    "test_df_original.loc[test_df_original['text_cleaned'].str.len() == 0, 'text_cleaned'] = 'empty'\n",
    "\n",
    "# Creating features for all test samples using the same vectorizer I fit on training data\n",
    "X_test_tfidf_full = tfidf.transform(test_df_original['text_cleaned'])\n",
    "test_df_original['text_length'] = test_df_original['text_cleaned'].apply(len)\n",
    "test_df_original['word_count'] = test_df_original['text_cleaned'].apply(lambda x: len(str(x).split()))\n",
    "test_df_original['avg_word_length'] = test_df_original['text_cleaned'].apply(\n",
    "    lambda x: np.mean([len(w) for w in str(x).split()]) if len(str(x).split()) > 0 else 0\n",
    ")\n",
    "test_basic_features_full = test_df_original[['text_length', 'word_count', 'avg_word_length']].values\n",
    "X_test_features_full = hstack([X_test_tfidf_full, test_basic_features_full])\n",
    "\n",
    "# Generating predictions for all test samples\n",
    "print(\"Generating predictions on test set...\")\n",
    "test_predictions_encoded = model.predict(X_test_features_full)\n",
    "# Converting back from numbers to emotion names\n",
    "test_predictions = label_encoder.inverse_transform(test_predictions_encoded)\n",
    "\n",
    "# Creating the submission file - needs to have id and emotion columns\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_df_original['id'].values,\n",
    "    'emotion': test_predictions\n",
    "})\n",
    "\n",
    "# Double-checking the submission has the right number of rows\n",
    "print(f\"\\nSubmission shape: {submission.shape}\")\n",
    "if len(submission) != 1680:\n",
    "    print(f\"ERROR: Submission should have 1680 rows, but has {len(submission)} rows\")\n",
    "    print(\"Check that test.csv has exactly 1680 rows\")\n",
    "else:\n",
    "    print(\"Submission has correct number of rows (1680)\")\n",
    "\n",
    "# Saving the submission file\n",
    "submission_path = 'submission.csv'\n",
    "submission.to_csv(submission_path, index=False)\n",
    "print(f\"\\nSubmission file saved to {submission_path}\")\n",
    "print(f\"\\nSubmission file preview:\")\n",
    "print(submission.head(10))\n",
    "\n",
    "# Checking the distribution of predictions - want to make sure it's reasonable\n",
    "print(f\"\\nSubmission statistics:\")\n",
    "print(f\"Total predictions: {len(submission)}\")\n",
    "print(f\"\\nPrediction distribution:\")\n",
    "print(submission['emotion'].value_counts())\n",
    "\n",
    "# Looking at some example predictions to see if they make sense\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"SAMPLE PREDICTIONS\")\n",
    "print(f\"{'='*60}\")\n",
    "for idx in range(min(5, len(test_df_original))):\n",
    "    print(f\"\\nText: {test_df_original.iloc[idx]['text_cleaned'][:80] if 'text_cleaned' in test_df_original.columns else test_df_original.iloc[idx]['text'][:80]}...\")\n",
    "    print(f\"Predicted: {test_predictions[idx]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Submit to Kaggle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submitting to Kaggle - fingers crossed!\n",
    "submission_path = 'submission.csv'\n",
    "\n",
    "try:\n",
    "    if Path(submission_path).exists():\n",
    "        print(f\"Submitting {submission_path} to Kaggle...\")\n",
    "        api.competition_submit(\n",
    "            file_name=submission_path,\n",
    "            message=\"Emotion recognition using TF-IDF + Ensemble\",\n",
    "            competition=competition_name\n",
    "        )\n",
    "        print(\"Submission successful!\")\n",
    "        \n",
    "        # Checking my recent submissions to see the results\n",
    "        print(\"\\nRecent submissions:\")\n",
    "        submissions = api.competition_submissions(competition_name)\n",
    "        for i, sub in enumerate(submissions[:3], 1):\n",
    "            print(f\"  {i}. {sub}\")\n",
    "    else:\n",
    "        print(f\"Submission file {submission_path} not found!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error submitting to Kaggle: {e}\")\n",
    "    print(\"You can submit manually at:\")\n",
    "    print(f\"  https://www.kaggle.com/competitions/{competition_name}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DM2025-Lab2-Exercise",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
